{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163c4491",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e224f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "print(torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e640e83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\dina\\anaconda3\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\dina\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\dina\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\dina\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\dina\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dina\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\dina\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dina\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\dina\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10b11b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dina\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Dina\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dcf55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c94260c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "num_epochs= 16\n",
    "batch_size =4\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "25ed1eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3d89287a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(root='./data',train=True,download=True, transform = transform )\n",
    "test_data = torchvision.datasets.CIFAR10(root='./data',train=False,download=True, transform = transform )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e2396429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "562dff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (\n",
    "    \"Airplane\",\n",
    "    \"Automobile\",\n",
    "    \"Bird\",\n",
    "    \"Cat\",\n",
    "    \"Deer\",\n",
    "    \"Dog\",\n",
    "    \"Frog\",\n",
    "    \"Horse\",\n",
    "    \"Ship\",\n",
    "    \"Truck\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b682d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implement conv net\n",
    "class ConvNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # define the layers of convolutional neural network\n",
    "        self.conv1 = nn.Conv2d(3, 6, kernel_size = 5)\n",
    "        #self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2, stride =2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size = 5)\n",
    "        \n",
    "        \n",
    "        # filter = 3*3 | input = 5*5 | padding =0| strid =1\n",
    "        # => (input-filter+2*padding)/strid+strid = 3\n",
    "        # => 3*3\n",
    "        # apply after all layers then take the input to \n",
    "        #fully connected layers\n",
    "        # this is implemented in last 3 numbers in images.size() \n",
    "        # here the out put is 16 * 5 * 5\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)  \n",
    "        self.fc3 = nn.Linear(84, 10)  \n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Define the forward pass through the network\n",
    "        x = self.maxpool(F.relu(self.conv1(x)))\n",
    "        x = self.maxpool(F.relu(self.conv2(x)))\n",
    "\n",
    "        \n",
    "        # Flatten the output before passing through fully connected layers\n",
    "        x = x.view(-1, 16*5*5)  # Adjust the size based on your network architecture\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "20549492",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ce48592e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/16], Step[200/16], Loss:2.2579\n",
      "Epoch[1/16], Step[400/16], Loss:2.3194\n",
      "Epoch[1/16], Step[600/16], Loss:2.2793\n",
      "Epoch[1/16], Step[800/16], Loss:2.2992\n",
      "Epoch[1/16], Step[1000/16], Loss:2.3202\n",
      "Epoch[1/16], Step[1200/16], Loss:2.2825\n",
      "Epoch[1/16], Step[1400/16], Loss:2.2681\n",
      "Epoch[1/16], Step[1600/16], Loss:2.2657\n",
      "Epoch[1/16], Step[1800/16], Loss:2.2635\n",
      "Epoch[1/16], Step[2000/16], Loss:2.3085\n",
      "Epoch[1/16], Step[2200/16], Loss:2.2901\n",
      "Epoch[1/16], Step[2400/16], Loss:2.3379\n",
      "Epoch[1/16], Step[2600/16], Loss:2.3383\n",
      "Epoch[1/16], Step[2800/16], Loss:2.2746\n",
      "Epoch[1/16], Step[3000/16], Loss:2.2834\n",
      "Epoch[1/16], Step[3200/16], Loss:2.2893\n",
      "Epoch[1/16], Step[3400/16], Loss:2.2959\n",
      "Epoch[1/16], Step[3600/16], Loss:2.3461\n",
      "Epoch[1/16], Step[3800/16], Loss:2.2777\n",
      "Epoch[1/16], Step[4000/16], Loss:2.2822\n",
      "Epoch[1/16], Step[4200/16], Loss:2.2962\n",
      "Epoch[1/16], Step[4400/16], Loss:2.3028\n",
      "Epoch[1/16], Step[4600/16], Loss:2.3219\n",
      "Epoch[1/16], Step[4800/16], Loss:2.3065\n",
      "Epoch[1/16], Step[5000/16], Loss:2.2726\n",
      "Epoch[1/16], Step[5200/16], Loss:2.2771\n",
      "Epoch[1/16], Step[5400/16], Loss:2.3031\n",
      "Epoch[1/16], Step[5600/16], Loss:2.3078\n",
      "Epoch[1/16], Step[5800/16], Loss:2.2919\n",
      "Epoch[1/16], Step[6000/16], Loss:2.2658\n",
      "Epoch[1/16], Step[6200/16], Loss:2.3122\n",
      "Epoch[1/16], Step[6400/16], Loss:2.2834\n",
      "Epoch[1/16], Step[6600/16], Loss:2.2886\n",
      "Epoch[1/16], Step[6800/16], Loss:2.3029\n",
      "Epoch[1/16], Step[7000/16], Loss:2.3107\n",
      "Epoch[1/16], Step[7200/16], Loss:2.2890\n",
      "Epoch[1/16], Step[7400/16], Loss:2.3114\n",
      "Epoch[1/16], Step[7600/16], Loss:2.3009\n",
      "Epoch[1/16], Step[7800/16], Loss:2.3093\n",
      "Epoch[1/16], Step[8000/16], Loss:2.3093\n",
      "Epoch[1/16], Step[8200/16], Loss:2.2900\n",
      "Epoch[1/16], Step[8400/16], Loss:2.2874\n",
      "Epoch[1/16], Step[8600/16], Loss:2.3035\n",
      "Epoch[1/16], Step[8800/16], Loss:2.3179\n",
      "Epoch[1/16], Step[9000/16], Loss:2.2772\n",
      "Epoch[1/16], Step[9200/16], Loss:2.2759\n",
      "Epoch[1/16], Step[9400/16], Loss:2.2952\n",
      "Epoch[1/16], Step[9600/16], Loss:2.2856\n",
      "Epoch[1/16], Step[9800/16], Loss:2.2914\n",
      "Epoch[1/16], Step[10000/16], Loss:2.2969\n",
      "Epoch[1/16], Step[10200/16], Loss:2.2653\n",
      "Epoch[1/16], Step[10400/16], Loss:2.2838\n",
      "Epoch[1/16], Step[10600/16], Loss:2.2733\n",
      "Epoch[1/16], Step[10800/16], Loss:2.2946\n",
      "Epoch[1/16], Step[11000/16], Loss:2.2641\n",
      "Epoch[1/16], Step[11200/16], Loss:2.2637\n",
      "Epoch[1/16], Step[11400/16], Loss:2.2403\n",
      "Epoch[1/16], Step[11600/16], Loss:2.1401\n",
      "Epoch[1/16], Step[11800/16], Loss:2.2676\n",
      "Epoch[1/16], Step[12000/16], Loss:2.3267\n",
      "Epoch[1/16], Step[12200/16], Loss:2.2375\n",
      "Epoch[1/16], Step[12400/16], Loss:2.2461\n",
      "Epoch[2/16], Step[200/16], Loss:2.2425\n",
      "Epoch[2/16], Step[400/16], Loss:2.0302\n",
      "Epoch[2/16], Step[600/16], Loss:2.1212\n",
      "Epoch[2/16], Step[800/16], Loss:2.0222\n",
      "Epoch[2/16], Step[1000/16], Loss:2.2621\n",
      "Epoch[2/16], Step[1200/16], Loss:2.2910\n",
      "Epoch[2/16], Step[1400/16], Loss:2.0822\n",
      "Epoch[2/16], Step[1600/16], Loss:2.3565\n",
      "Epoch[2/16], Step[1800/16], Loss:2.1266\n",
      "Epoch[2/16], Step[2000/16], Loss:2.3868\n",
      "Epoch[2/16], Step[2200/16], Loss:1.8911\n",
      "Epoch[2/16], Step[2400/16], Loss:2.2420\n",
      "Epoch[2/16], Step[2600/16], Loss:2.0806\n",
      "Epoch[2/16], Step[2800/16], Loss:2.0539\n",
      "Epoch[2/16], Step[3000/16], Loss:1.9137\n",
      "Epoch[2/16], Step[3200/16], Loss:2.0839\n",
      "Epoch[2/16], Step[3400/16], Loss:1.8907\n",
      "Epoch[2/16], Step[3600/16], Loss:1.8222\n",
      "Epoch[2/16], Step[3800/16], Loss:1.8737\n",
      "Epoch[2/16], Step[4000/16], Loss:2.0189\n",
      "Epoch[2/16], Step[4200/16], Loss:1.6370\n",
      "Epoch[2/16], Step[4400/16], Loss:2.4989\n",
      "Epoch[2/16], Step[4600/16], Loss:2.0280\n",
      "Epoch[2/16], Step[4800/16], Loss:1.9028\n",
      "Epoch[2/16], Step[5000/16], Loss:1.9992\n",
      "Epoch[2/16], Step[5200/16], Loss:2.1490\n",
      "Epoch[2/16], Step[5400/16], Loss:2.9955\n",
      "Epoch[2/16], Step[5600/16], Loss:2.1712\n",
      "Epoch[2/16], Step[5800/16], Loss:1.7126\n",
      "Epoch[2/16], Step[6000/16], Loss:3.0713\n",
      "Epoch[2/16], Step[6200/16], Loss:2.0389\n",
      "Epoch[2/16], Step[6400/16], Loss:2.0563\n",
      "Epoch[2/16], Step[6600/16], Loss:1.5518\n",
      "Epoch[2/16], Step[6800/16], Loss:2.0806\n",
      "Epoch[2/16], Step[7000/16], Loss:1.8186\n",
      "Epoch[2/16], Step[7200/16], Loss:1.5542\n",
      "Epoch[2/16], Step[7400/16], Loss:1.8701\n",
      "Epoch[2/16], Step[7600/16], Loss:1.5749\n",
      "Epoch[2/16], Step[7800/16], Loss:2.7609\n",
      "Epoch[2/16], Step[8000/16], Loss:1.9526\n",
      "Epoch[2/16], Step[8200/16], Loss:1.3552\n",
      "Epoch[2/16], Step[8400/16], Loss:2.6008\n",
      "Epoch[2/16], Step[8600/16], Loss:1.7824\n",
      "Epoch[2/16], Step[8800/16], Loss:1.8267\n",
      "Epoch[2/16], Step[9000/16], Loss:2.7320\n",
      "Epoch[2/16], Step[9200/16], Loss:2.0404\n",
      "Epoch[2/16], Step[9400/16], Loss:2.5698\n",
      "Epoch[2/16], Step[9600/16], Loss:2.3806\n",
      "Epoch[2/16], Step[9800/16], Loss:1.7968\n",
      "Epoch[2/16], Step[10000/16], Loss:2.0342\n",
      "Epoch[2/16], Step[10200/16], Loss:1.7888\n",
      "Epoch[2/16], Step[10400/16], Loss:1.6343\n",
      "Epoch[2/16], Step[10600/16], Loss:2.5867\n",
      "Epoch[2/16], Step[10800/16], Loss:2.0002\n",
      "Epoch[2/16], Step[11000/16], Loss:2.1059\n",
      "Epoch[2/16], Step[11200/16], Loss:2.0031\n",
      "Epoch[2/16], Step[11400/16], Loss:2.2863\n",
      "Epoch[2/16], Step[11600/16], Loss:1.9830\n",
      "Epoch[2/16], Step[11800/16], Loss:1.5339\n",
      "Epoch[2/16], Step[12000/16], Loss:1.4781\n",
      "Epoch[2/16], Step[12200/16], Loss:1.8051\n",
      "Epoch[2/16], Step[12400/16], Loss:1.3605\n",
      "Epoch[3/16], Step[200/16], Loss:2.0052\n",
      "Epoch[3/16], Step[400/16], Loss:1.7415\n",
      "Epoch[3/16], Step[600/16], Loss:2.3915\n",
      "Epoch[3/16], Step[800/16], Loss:1.8809\n",
      "Epoch[3/16], Step[1000/16], Loss:1.4480\n",
      "Epoch[3/16], Step[1200/16], Loss:1.6546\n",
      "Epoch[3/16], Step[1400/16], Loss:0.9410\n",
      "Epoch[3/16], Step[1600/16], Loss:1.4535\n",
      "Epoch[3/16], Step[1800/16], Loss:2.1904\n",
      "Epoch[3/16], Step[2000/16], Loss:2.7727\n",
      "Epoch[3/16], Step[2200/16], Loss:1.8045\n",
      "Epoch[3/16], Step[2400/16], Loss:1.6982\n",
      "Epoch[3/16], Step[2600/16], Loss:1.5719\n",
      "Epoch[3/16], Step[2800/16], Loss:1.5746\n",
      "Epoch[3/16], Step[3000/16], Loss:1.5436\n",
      "Epoch[3/16], Step[3200/16], Loss:3.0853\n",
      "Epoch[3/16], Step[3400/16], Loss:3.2931\n",
      "Epoch[3/16], Step[3600/16], Loss:1.6551\n",
      "Epoch[3/16], Step[3800/16], Loss:1.9850\n",
      "Epoch[3/16], Step[4000/16], Loss:2.8952\n",
      "Epoch[3/16], Step[4200/16], Loss:1.8487\n",
      "Epoch[3/16], Step[4400/16], Loss:2.3422\n",
      "Epoch[3/16], Step[4600/16], Loss:1.3059\n",
      "Epoch[3/16], Step[4800/16], Loss:1.6174\n",
      "Epoch[3/16], Step[5000/16], Loss:2.1378\n",
      "Epoch[3/16], Step[5200/16], Loss:1.2731\n",
      "Epoch[3/16], Step[5400/16], Loss:1.8114\n",
      "Epoch[3/16], Step[5600/16], Loss:1.9641\n",
      "Epoch[3/16], Step[5800/16], Loss:1.4247\n",
      "Epoch[3/16], Step[6000/16], Loss:2.2617\n",
      "Epoch[3/16], Step[6200/16], Loss:1.5830\n",
      "Epoch[3/16], Step[6400/16], Loss:1.9005\n",
      "Epoch[3/16], Step[6600/16], Loss:2.0093\n",
      "Epoch[3/16], Step[6800/16], Loss:2.0064\n",
      "Epoch[3/16], Step[7000/16], Loss:1.5623\n",
      "Epoch[3/16], Step[7200/16], Loss:1.4080\n",
      "Epoch[3/16], Step[7400/16], Loss:1.6653\n",
      "Epoch[3/16], Step[7600/16], Loss:2.3198\n",
      "Epoch[3/16], Step[7800/16], Loss:2.5095\n",
      "Epoch[3/16], Step[8000/16], Loss:2.1667\n",
      "Epoch[3/16], Step[8200/16], Loss:1.5900\n",
      "Epoch[3/16], Step[8400/16], Loss:2.2935\n",
      "Epoch[3/16], Step[8600/16], Loss:2.0846\n",
      "Epoch[3/16], Step[8800/16], Loss:1.0145\n",
      "Epoch[3/16], Step[9000/16], Loss:0.9982\n",
      "Epoch[3/16], Step[9200/16], Loss:1.3484\n",
      "Epoch[3/16], Step[9400/16], Loss:1.4114\n",
      "Epoch[3/16], Step[9600/16], Loss:2.3719\n",
      "Epoch[3/16], Step[9800/16], Loss:1.9103\n",
      "Epoch[3/16], Step[10000/16], Loss:1.0403\n",
      "Epoch[3/16], Step[10200/16], Loss:1.4641\n",
      "Epoch[3/16], Step[10400/16], Loss:1.9141\n",
      "Epoch[3/16], Step[10600/16], Loss:1.9636\n",
      "Epoch[3/16], Step[10800/16], Loss:1.6333\n",
      "Epoch[3/16], Step[11000/16], Loss:2.0022\n",
      "Epoch[3/16], Step[11200/16], Loss:1.6993\n",
      "Epoch[3/16], Step[11400/16], Loss:2.2297\n",
      "Epoch[3/16], Step[11600/16], Loss:1.9949\n",
      "Epoch[3/16], Step[11800/16], Loss:1.2813\n",
      "Epoch[3/16], Step[12000/16], Loss:1.3864\n",
      "Epoch[3/16], Step[12200/16], Loss:1.4869\n",
      "Epoch[3/16], Step[12400/16], Loss:1.8064\n",
      "Epoch[4/16], Step[200/16], Loss:1.8907\n",
      "Epoch[4/16], Step[400/16], Loss:2.4277\n",
      "Epoch[4/16], Step[600/16], Loss:1.3876\n",
      "Epoch[4/16], Step[800/16], Loss:1.8766\n",
      "Epoch[4/16], Step[1000/16], Loss:1.4399\n",
      "Epoch[4/16], Step[1200/16], Loss:2.1234\n",
      "Epoch[4/16], Step[1400/16], Loss:2.7468\n",
      "Epoch[4/16], Step[1600/16], Loss:2.2857\n",
      "Epoch[4/16], Step[1800/16], Loss:1.2306\n",
      "Epoch[4/16], Step[2000/16], Loss:1.0450\n",
      "Epoch[4/16], Step[2200/16], Loss:2.4227\n",
      "Epoch[4/16], Step[2400/16], Loss:2.1619\n",
      "Epoch[4/16], Step[2600/16], Loss:1.6351\n",
      "Epoch[4/16], Step[2800/16], Loss:3.1712\n",
      "Epoch[4/16], Step[3000/16], Loss:1.6277\n",
      "Epoch[4/16], Step[3200/16], Loss:0.6207\n",
      "Epoch[4/16], Step[3400/16], Loss:1.4906\n",
      "Epoch[4/16], Step[3600/16], Loss:1.5973\n",
      "Epoch[4/16], Step[3800/16], Loss:1.2811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[4/16], Step[4000/16], Loss:2.3734\n",
      "Epoch[4/16], Step[4200/16], Loss:2.1163\n",
      "Epoch[4/16], Step[4400/16], Loss:1.9728\n",
      "Epoch[4/16], Step[4600/16], Loss:1.6417\n",
      "Epoch[4/16], Step[4800/16], Loss:0.8831\n",
      "Epoch[4/16], Step[5000/16], Loss:1.5776\n",
      "Epoch[4/16], Step[5200/16], Loss:1.7535\n",
      "Epoch[4/16], Step[5400/16], Loss:1.4140\n",
      "Epoch[4/16], Step[5600/16], Loss:2.4914\n",
      "Epoch[4/16], Step[5800/16], Loss:1.0691\n",
      "Epoch[4/16], Step[6000/16], Loss:2.4186\n",
      "Epoch[4/16], Step[6200/16], Loss:1.4871\n",
      "Epoch[4/16], Step[6400/16], Loss:1.2973\n",
      "Epoch[4/16], Step[6600/16], Loss:1.2132\n",
      "Epoch[4/16], Step[6800/16], Loss:1.1901\n",
      "Epoch[4/16], Step[7000/16], Loss:1.2387\n",
      "Epoch[4/16], Step[7200/16], Loss:3.2390\n",
      "Epoch[4/16], Step[7400/16], Loss:1.8454\n",
      "Epoch[4/16], Step[7600/16], Loss:1.6127\n",
      "Epoch[4/16], Step[7800/16], Loss:1.5828\n",
      "Epoch[4/16], Step[8000/16], Loss:1.9988\n",
      "Epoch[4/16], Step[8200/16], Loss:1.4107\n",
      "Epoch[4/16], Step[8400/16], Loss:1.0678\n",
      "Epoch[4/16], Step[8600/16], Loss:1.1303\n",
      "Epoch[4/16], Step[8800/16], Loss:0.9780\n",
      "Epoch[4/16], Step[9000/16], Loss:0.9982\n",
      "Epoch[4/16], Step[9200/16], Loss:2.1884\n",
      "Epoch[4/16], Step[9400/16], Loss:1.6659\n",
      "Epoch[4/16], Step[9600/16], Loss:1.0801\n",
      "Epoch[4/16], Step[9800/16], Loss:1.6040\n",
      "Epoch[4/16], Step[10000/16], Loss:1.4936\n",
      "Epoch[4/16], Step[10200/16], Loss:1.1660\n",
      "Epoch[4/16], Step[10400/16], Loss:1.5510\n",
      "Epoch[4/16], Step[10600/16], Loss:2.1990\n",
      "Epoch[4/16], Step[10800/16], Loss:0.9099\n",
      "Epoch[4/16], Step[11000/16], Loss:2.5759\n",
      "Epoch[4/16], Step[11200/16], Loss:2.3187\n",
      "Epoch[4/16], Step[11400/16], Loss:0.8380\n",
      "Epoch[4/16], Step[11600/16], Loss:0.8107\n",
      "Epoch[4/16], Step[11800/16], Loss:1.7145\n",
      "Epoch[4/16], Step[12000/16], Loss:1.7922\n",
      "Epoch[4/16], Step[12200/16], Loss:1.3815\n",
      "Epoch[4/16], Step[12400/16], Loss:1.0210\n",
      "Epoch[5/16], Step[200/16], Loss:2.1945\n",
      "Epoch[5/16], Step[400/16], Loss:1.4022\n",
      "Epoch[5/16], Step[600/16], Loss:1.6416\n",
      "Epoch[5/16], Step[800/16], Loss:1.4664\n",
      "Epoch[5/16], Step[1000/16], Loss:1.4847\n",
      "Epoch[5/16], Step[1200/16], Loss:0.9735\n",
      "Epoch[5/16], Step[1400/16], Loss:0.4022\n",
      "Epoch[5/16], Step[1600/16], Loss:0.9467\n",
      "Epoch[5/16], Step[1800/16], Loss:2.4765\n",
      "Epoch[5/16], Step[2000/16], Loss:2.1523\n",
      "Epoch[5/16], Step[2200/16], Loss:1.3937\n",
      "Epoch[5/16], Step[2400/16], Loss:3.2430\n",
      "Epoch[5/16], Step[2600/16], Loss:2.4383\n",
      "Epoch[5/16], Step[2800/16], Loss:1.6635\n",
      "Epoch[5/16], Step[3000/16], Loss:1.8080\n",
      "Epoch[5/16], Step[3200/16], Loss:2.0732\n",
      "Epoch[5/16], Step[3400/16], Loss:1.0333\n",
      "Epoch[5/16], Step[3600/16], Loss:1.8821\n",
      "Epoch[5/16], Step[3800/16], Loss:1.1495\n",
      "Epoch[5/16], Step[4000/16], Loss:0.6588\n",
      "Epoch[5/16], Step[4200/16], Loss:2.1551\n",
      "Epoch[5/16], Step[4400/16], Loss:1.2872\n",
      "Epoch[5/16], Step[4600/16], Loss:1.1447\n",
      "Epoch[5/16], Step[4800/16], Loss:2.3485\n",
      "Epoch[5/16], Step[5000/16], Loss:2.0290\n",
      "Epoch[5/16], Step[5200/16], Loss:1.4116\n",
      "Epoch[5/16], Step[5400/16], Loss:0.5336\n",
      "Epoch[5/16], Step[5600/16], Loss:1.7258\n",
      "Epoch[5/16], Step[5800/16], Loss:1.6513\n",
      "Epoch[5/16], Step[6000/16], Loss:1.7627\n",
      "Epoch[5/16], Step[6200/16], Loss:1.1559\n",
      "Epoch[5/16], Step[6400/16], Loss:1.1816\n",
      "Epoch[5/16], Step[6600/16], Loss:1.8158\n",
      "Epoch[5/16], Step[6800/16], Loss:1.5912\n",
      "Epoch[5/16], Step[7000/16], Loss:1.8047\n",
      "Epoch[5/16], Step[7200/16], Loss:1.3234\n",
      "Epoch[5/16], Step[7400/16], Loss:1.0949\n",
      "Epoch[5/16], Step[7600/16], Loss:2.0070\n",
      "Epoch[5/16], Step[7800/16], Loss:2.0628\n",
      "Epoch[5/16], Step[8000/16], Loss:1.0944\n",
      "Epoch[5/16], Step[8200/16], Loss:1.3938\n",
      "Epoch[5/16], Step[8400/16], Loss:1.1887\n",
      "Epoch[5/16], Step[8600/16], Loss:0.8129\n",
      "Epoch[5/16], Step[8800/16], Loss:1.7333\n",
      "Epoch[5/16], Step[9000/16], Loss:1.4334\n",
      "Epoch[5/16], Step[9200/16], Loss:1.9813\n",
      "Epoch[5/16], Step[9400/16], Loss:1.0429\n",
      "Epoch[5/16], Step[9600/16], Loss:1.2916\n",
      "Epoch[5/16], Step[9800/16], Loss:2.0626\n",
      "Epoch[5/16], Step[10000/16], Loss:1.2197\n",
      "Epoch[5/16], Step[10200/16], Loss:1.8534\n",
      "Epoch[5/16], Step[10400/16], Loss:1.2360\n",
      "Epoch[5/16], Step[10600/16], Loss:1.5595\n",
      "Epoch[5/16], Step[10800/16], Loss:1.6245\n",
      "Epoch[5/16], Step[11000/16], Loss:0.8155\n",
      "Epoch[5/16], Step[11200/16], Loss:0.9486\n",
      "Epoch[5/16], Step[11400/16], Loss:0.5055\n",
      "Epoch[5/16], Step[11600/16], Loss:1.1472\n",
      "Epoch[5/16], Step[11800/16], Loss:0.7610\n",
      "Epoch[5/16], Step[12000/16], Loss:0.8889\n",
      "Epoch[5/16], Step[12200/16], Loss:1.3075\n",
      "Epoch[5/16], Step[12400/16], Loss:1.3374\n",
      "Epoch[6/16], Step[200/16], Loss:1.5357\n",
      "Epoch[6/16], Step[400/16], Loss:0.6635\n",
      "Epoch[6/16], Step[600/16], Loss:1.2400\n",
      "Epoch[6/16], Step[800/16], Loss:1.3440\n",
      "Epoch[6/16], Step[1000/16], Loss:2.0619\n",
      "Epoch[6/16], Step[1200/16], Loss:0.8053\n",
      "Epoch[6/16], Step[1400/16], Loss:0.8288\n",
      "Epoch[6/16], Step[1600/16], Loss:0.5835\n",
      "Epoch[6/16], Step[1800/16], Loss:1.6729\n",
      "Epoch[6/16], Step[2000/16], Loss:1.3048\n",
      "Epoch[6/16], Step[2200/16], Loss:1.0413\n",
      "Epoch[6/16], Step[2400/16], Loss:0.9315\n",
      "Epoch[6/16], Step[2600/16], Loss:1.6018\n",
      "Epoch[6/16], Step[2800/16], Loss:0.9391\n",
      "Epoch[6/16], Step[3000/16], Loss:1.7962\n",
      "Epoch[6/16], Step[3200/16], Loss:1.5811\n",
      "Epoch[6/16], Step[3400/16], Loss:0.6749\n",
      "Epoch[6/16], Step[3600/16], Loss:0.9133\n",
      "Epoch[6/16], Step[3800/16], Loss:1.3899\n",
      "Epoch[6/16], Step[4000/16], Loss:1.4962\n",
      "Epoch[6/16], Step[4200/16], Loss:1.7664\n",
      "Epoch[6/16], Step[4400/16], Loss:0.9622\n",
      "Epoch[6/16], Step[4600/16], Loss:1.7715\n",
      "Epoch[6/16], Step[4800/16], Loss:0.9309\n",
      "Epoch[6/16], Step[5000/16], Loss:1.4187\n",
      "Epoch[6/16], Step[5200/16], Loss:1.5962\n",
      "Epoch[6/16], Step[5400/16], Loss:1.0391\n",
      "Epoch[6/16], Step[5600/16], Loss:2.2458\n",
      "Epoch[6/16], Step[5800/16], Loss:1.0076\n",
      "Epoch[6/16], Step[6000/16], Loss:1.6009\n",
      "Epoch[6/16], Step[6200/16], Loss:1.9959\n",
      "Epoch[6/16], Step[6400/16], Loss:0.6737\n",
      "Epoch[6/16], Step[6600/16], Loss:1.9716\n",
      "Epoch[6/16], Step[6800/16], Loss:0.8513\n",
      "Epoch[6/16], Step[7000/16], Loss:1.3529\n",
      "Epoch[6/16], Step[7200/16], Loss:2.3777\n",
      "Epoch[6/16], Step[7400/16], Loss:0.9357\n",
      "Epoch[6/16], Step[7600/16], Loss:0.8292\n",
      "Epoch[6/16], Step[7800/16], Loss:1.3483\n",
      "Epoch[6/16], Step[8000/16], Loss:1.2408\n",
      "Epoch[6/16], Step[8200/16], Loss:1.1310\n",
      "Epoch[6/16], Step[8400/16], Loss:0.7521\n",
      "Epoch[6/16], Step[8600/16], Loss:1.9060\n",
      "Epoch[6/16], Step[8800/16], Loss:1.1336\n",
      "Epoch[6/16], Step[9000/16], Loss:1.0309\n",
      "Epoch[6/16], Step[9200/16], Loss:0.5866\n",
      "Epoch[6/16], Step[9400/16], Loss:1.8423\n",
      "Epoch[6/16], Step[9600/16], Loss:1.2392\n",
      "Epoch[6/16], Step[9800/16], Loss:1.2874\n",
      "Epoch[6/16], Step[10000/16], Loss:1.3585\n",
      "Epoch[6/16], Step[10200/16], Loss:1.3266\n",
      "Epoch[6/16], Step[10400/16], Loss:0.9489\n",
      "Epoch[6/16], Step[10600/16], Loss:1.2560\n",
      "Epoch[6/16], Step[10800/16], Loss:1.0169\n",
      "Epoch[6/16], Step[11000/16], Loss:2.3174\n",
      "Epoch[6/16], Step[11200/16], Loss:1.7165\n",
      "Epoch[6/16], Step[11400/16], Loss:1.5841\n",
      "Epoch[6/16], Step[11600/16], Loss:1.0900\n",
      "Epoch[6/16], Step[11800/16], Loss:1.4680\n",
      "Epoch[6/16], Step[12000/16], Loss:1.5865\n",
      "Epoch[6/16], Step[12200/16], Loss:1.1966\n",
      "Epoch[6/16], Step[12400/16], Loss:0.4791\n",
      "Epoch[7/16], Step[200/16], Loss:1.1436\n",
      "Epoch[7/16], Step[400/16], Loss:1.5355\n",
      "Epoch[7/16], Step[600/16], Loss:0.8164\n",
      "Epoch[7/16], Step[800/16], Loss:1.4089\n",
      "Epoch[7/16], Step[1000/16], Loss:1.5929\n",
      "Epoch[7/16], Step[1200/16], Loss:1.0866\n",
      "Epoch[7/16], Step[1400/16], Loss:1.3790\n",
      "Epoch[7/16], Step[1600/16], Loss:1.4060\n",
      "Epoch[7/16], Step[1800/16], Loss:1.2618\n",
      "Epoch[7/16], Step[2000/16], Loss:1.9373\n",
      "Epoch[7/16], Step[2200/16], Loss:0.2819\n",
      "Epoch[7/16], Step[2400/16], Loss:1.7010\n",
      "Epoch[7/16], Step[2600/16], Loss:1.2467\n",
      "Epoch[7/16], Step[2800/16], Loss:1.4307\n",
      "Epoch[7/16], Step[3000/16], Loss:0.6481\n",
      "Epoch[7/16], Step[3200/16], Loss:1.0251\n",
      "Epoch[7/16], Step[3400/16], Loss:1.3621\n",
      "Epoch[7/16], Step[3600/16], Loss:1.1873\n",
      "Epoch[7/16], Step[3800/16], Loss:0.3514\n",
      "Epoch[7/16], Step[4000/16], Loss:1.4716\n",
      "Epoch[7/16], Step[4200/16], Loss:0.4935\n",
      "Epoch[7/16], Step[4400/16], Loss:1.3408\n",
      "Epoch[7/16], Step[4600/16], Loss:0.9066\n",
      "Epoch[7/16], Step[4800/16], Loss:2.4523\n",
      "Epoch[7/16], Step[5000/16], Loss:1.6292\n",
      "Epoch[7/16], Step[5200/16], Loss:2.0090\n",
      "Epoch[7/16], Step[5400/16], Loss:1.5837\n",
      "Epoch[7/16], Step[5600/16], Loss:0.8272\n",
      "Epoch[7/16], Step[5800/16], Loss:1.2161\n",
      "Epoch[7/16], Step[6000/16], Loss:1.2166\n",
      "Epoch[7/16], Step[6200/16], Loss:1.2134\n",
      "Epoch[7/16], Step[6400/16], Loss:1.7933\n",
      "Epoch[7/16], Step[6600/16], Loss:1.3292\n",
      "Epoch[7/16], Step[6800/16], Loss:0.7641\n",
      "Epoch[7/16], Step[7000/16], Loss:0.6043\n",
      "Epoch[7/16], Step[7200/16], Loss:2.0747\n",
      "Epoch[7/16], Step[7400/16], Loss:2.1552\n",
      "Epoch[7/16], Step[7600/16], Loss:1.2476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[7/16], Step[7800/16], Loss:1.0484\n",
      "Epoch[7/16], Step[8000/16], Loss:1.0260\n",
      "Epoch[7/16], Step[8200/16], Loss:1.1848\n",
      "Epoch[7/16], Step[8400/16], Loss:3.2067\n",
      "Epoch[7/16], Step[8600/16], Loss:0.8356\n",
      "Epoch[7/16], Step[8800/16], Loss:1.4202\n",
      "Epoch[7/16], Step[9000/16], Loss:1.8655\n",
      "Epoch[7/16], Step[9200/16], Loss:1.1315\n",
      "Epoch[7/16], Step[9400/16], Loss:0.8506\n",
      "Epoch[7/16], Step[9600/16], Loss:1.5294\n",
      "Epoch[7/16], Step[9800/16], Loss:1.8938\n",
      "Epoch[7/16], Step[10000/16], Loss:0.6283\n",
      "Epoch[7/16], Step[10200/16], Loss:1.4155\n",
      "Epoch[7/16], Step[10400/16], Loss:1.9292\n",
      "Epoch[7/16], Step[10600/16], Loss:1.1911\n",
      "Epoch[7/16], Step[10800/16], Loss:1.3617\n",
      "Epoch[7/16], Step[11000/16], Loss:3.2200\n",
      "Epoch[7/16], Step[11200/16], Loss:0.9760\n",
      "Epoch[7/16], Step[11400/16], Loss:1.0753\n",
      "Epoch[7/16], Step[11600/16], Loss:1.0870\n",
      "Epoch[7/16], Step[11800/16], Loss:1.2756\n",
      "Epoch[7/16], Step[12000/16], Loss:1.4134\n",
      "Epoch[7/16], Step[12200/16], Loss:1.8233\n",
      "Epoch[7/16], Step[12400/16], Loss:1.4580\n",
      "Epoch[8/16], Step[200/16], Loss:1.9073\n",
      "Epoch[8/16], Step[400/16], Loss:1.1903\n",
      "Epoch[8/16], Step[600/16], Loss:1.1452\n",
      "Epoch[8/16], Step[800/16], Loss:0.9085\n",
      "Epoch[8/16], Step[1000/16], Loss:1.1307\n",
      "Epoch[8/16], Step[1200/16], Loss:1.5119\n",
      "Epoch[8/16], Step[1400/16], Loss:0.9188\n",
      "Epoch[8/16], Step[1600/16], Loss:1.2383\n",
      "Epoch[8/16], Step[1800/16], Loss:0.6696\n",
      "Epoch[8/16], Step[2000/16], Loss:1.7195\n",
      "Epoch[8/16], Step[2200/16], Loss:1.6427\n",
      "Epoch[8/16], Step[2400/16], Loss:1.3758\n",
      "Epoch[8/16], Step[2600/16], Loss:0.9059\n",
      "Epoch[8/16], Step[2800/16], Loss:0.8741\n",
      "Epoch[8/16], Step[3000/16], Loss:1.3411\n",
      "Epoch[8/16], Step[3200/16], Loss:1.1379\n",
      "Epoch[8/16], Step[3400/16], Loss:1.2857\n",
      "Epoch[8/16], Step[3600/16], Loss:1.9005\n",
      "Epoch[8/16], Step[3800/16], Loss:0.6822\n",
      "Epoch[8/16], Step[4000/16], Loss:0.6393\n",
      "Epoch[8/16], Step[4200/16], Loss:1.2728\n",
      "Epoch[8/16], Step[4400/16], Loss:1.8716\n",
      "Epoch[8/16], Step[4600/16], Loss:1.4710\n",
      "Epoch[8/16], Step[4800/16], Loss:1.0903\n",
      "Epoch[8/16], Step[5000/16], Loss:1.1706\n",
      "Epoch[8/16], Step[5200/16], Loss:1.4905\n",
      "Epoch[8/16], Step[5400/16], Loss:1.5123\n",
      "Epoch[8/16], Step[5600/16], Loss:1.4974\n",
      "Epoch[8/16], Step[5800/16], Loss:0.5369\n",
      "Epoch[8/16], Step[6000/16], Loss:1.2016\n",
      "Epoch[8/16], Step[6200/16], Loss:1.0846\n",
      "Epoch[8/16], Step[6400/16], Loss:1.0985\n",
      "Epoch[8/16], Step[6600/16], Loss:1.1037\n",
      "Epoch[8/16], Step[6800/16], Loss:2.5628\n",
      "Epoch[8/16], Step[7000/16], Loss:1.1719\n",
      "Epoch[8/16], Step[7200/16], Loss:2.1148\n",
      "Epoch[8/16], Step[7400/16], Loss:1.5689\n",
      "Epoch[8/16], Step[7600/16], Loss:1.4154\n",
      "Epoch[8/16], Step[7800/16], Loss:0.8094\n",
      "Epoch[8/16], Step[8000/16], Loss:1.2435\n",
      "Epoch[8/16], Step[8200/16], Loss:1.5707\n",
      "Epoch[8/16], Step[8400/16], Loss:0.5634\n",
      "Epoch[8/16], Step[8600/16], Loss:1.3979\n",
      "Epoch[8/16], Step[8800/16], Loss:1.3508\n",
      "Epoch[8/16], Step[9000/16], Loss:0.9653\n",
      "Epoch[8/16], Step[9200/16], Loss:0.6048\n",
      "Epoch[8/16], Step[9400/16], Loss:1.3391\n",
      "Epoch[8/16], Step[9600/16], Loss:1.1483\n",
      "Epoch[8/16], Step[9800/16], Loss:0.7238\n",
      "Epoch[8/16], Step[10000/16], Loss:1.1903\n",
      "Epoch[8/16], Step[10200/16], Loss:1.0045\n",
      "Epoch[8/16], Step[10400/16], Loss:1.0090\n",
      "Epoch[8/16], Step[10600/16], Loss:1.9750\n",
      "Epoch[8/16], Step[10800/16], Loss:1.1314\n",
      "Epoch[8/16], Step[11000/16], Loss:2.0965\n",
      "Epoch[8/16], Step[11200/16], Loss:1.6022\n",
      "Epoch[8/16], Step[11400/16], Loss:1.9642\n",
      "Epoch[8/16], Step[11600/16], Loss:1.5179\n",
      "Epoch[8/16], Step[11800/16], Loss:1.5603\n",
      "Epoch[8/16], Step[12000/16], Loss:1.6986\n",
      "Epoch[8/16], Step[12200/16], Loss:0.8169\n",
      "Epoch[8/16], Step[12400/16], Loss:0.8408\n",
      "Epoch[9/16], Step[200/16], Loss:0.6190\n",
      "Epoch[9/16], Step[400/16], Loss:0.4171\n",
      "Epoch[9/16], Step[600/16], Loss:2.2974\n",
      "Epoch[9/16], Step[800/16], Loss:1.7120\n",
      "Epoch[9/16], Step[1000/16], Loss:1.8338\n",
      "Epoch[9/16], Step[1200/16], Loss:1.4288\n",
      "Epoch[9/16], Step[1400/16], Loss:1.2640\n",
      "Epoch[9/16], Step[1600/16], Loss:1.2010\n",
      "Epoch[9/16], Step[1800/16], Loss:0.8849\n",
      "Epoch[9/16], Step[2000/16], Loss:1.6308\n",
      "Epoch[9/16], Step[2200/16], Loss:0.9649\n",
      "Epoch[9/16], Step[2400/16], Loss:0.9916\n",
      "Epoch[9/16], Step[2600/16], Loss:2.1622\n",
      "Epoch[9/16], Step[2800/16], Loss:0.8692\n",
      "Epoch[9/16], Step[3000/16], Loss:0.8879\n",
      "Epoch[9/16], Step[3200/16], Loss:1.0740\n",
      "Epoch[9/16], Step[3400/16], Loss:1.2512\n",
      "Epoch[9/16], Step[3600/16], Loss:1.2946\n",
      "Epoch[9/16], Step[3800/16], Loss:1.4581\n",
      "Epoch[9/16], Step[4000/16], Loss:0.9588\n",
      "Epoch[9/16], Step[4200/16], Loss:0.9912\n",
      "Epoch[9/16], Step[4400/16], Loss:1.2805\n",
      "Epoch[9/16], Step[4600/16], Loss:1.0681\n",
      "Epoch[9/16], Step[4800/16], Loss:0.6779\n",
      "Epoch[9/16], Step[5000/16], Loss:1.3942\n",
      "Epoch[9/16], Step[5200/16], Loss:2.0390\n",
      "Epoch[9/16], Step[5400/16], Loss:2.2202\n",
      "Epoch[9/16], Step[5600/16], Loss:1.2464\n",
      "Epoch[9/16], Step[5800/16], Loss:0.3079\n",
      "Epoch[9/16], Step[6000/16], Loss:0.7818\n",
      "Epoch[9/16], Step[6200/16], Loss:1.1565\n",
      "Epoch[9/16], Step[6400/16], Loss:0.6904\n",
      "Epoch[9/16], Step[6600/16], Loss:1.4934\n",
      "Epoch[9/16], Step[6800/16], Loss:1.5780\n",
      "Epoch[9/16], Step[7000/16], Loss:1.5366\n",
      "Epoch[9/16], Step[7200/16], Loss:0.5521\n",
      "Epoch[9/16], Step[7400/16], Loss:0.8551\n",
      "Epoch[9/16], Step[7600/16], Loss:0.7410\n",
      "Epoch[9/16], Step[7800/16], Loss:0.6674\n",
      "Epoch[9/16], Step[8000/16], Loss:0.8787\n",
      "Epoch[9/16], Step[8200/16], Loss:2.0980\n",
      "Epoch[9/16], Step[8400/16], Loss:0.6862\n",
      "Epoch[9/16], Step[8600/16], Loss:0.7343\n",
      "Epoch[9/16], Step[8800/16], Loss:1.7858\n",
      "Epoch[9/16], Step[9000/16], Loss:2.0537\n",
      "Epoch[9/16], Step[9200/16], Loss:0.5473\n",
      "Epoch[9/16], Step[9400/16], Loss:0.8265\n",
      "Epoch[9/16], Step[9600/16], Loss:0.9692\n",
      "Epoch[9/16], Step[9800/16], Loss:1.6629\n",
      "Epoch[9/16], Step[10000/16], Loss:1.4551\n",
      "Epoch[9/16], Step[10200/16], Loss:2.5451\n",
      "Epoch[9/16], Step[10400/16], Loss:0.8555\n",
      "Epoch[9/16], Step[10600/16], Loss:0.6975\n",
      "Epoch[9/16], Step[10800/16], Loss:1.7062\n",
      "Epoch[9/16], Step[11000/16], Loss:0.6804\n",
      "Epoch[9/16], Step[11200/16], Loss:0.7728\n",
      "Epoch[9/16], Step[11400/16], Loss:0.7625\n",
      "Epoch[9/16], Step[11600/16], Loss:0.8181\n",
      "Epoch[9/16], Step[11800/16], Loss:1.6273\n",
      "Epoch[9/16], Step[12000/16], Loss:0.9198\n",
      "Epoch[9/16], Step[12200/16], Loss:1.3656\n",
      "Epoch[9/16], Step[12400/16], Loss:1.1944\n",
      "Epoch[10/16], Step[200/16], Loss:1.4823\n",
      "Epoch[10/16], Step[400/16], Loss:1.6697\n",
      "Epoch[10/16], Step[600/16], Loss:2.3603\n",
      "Epoch[10/16], Step[800/16], Loss:2.4042\n",
      "Epoch[10/16], Step[1000/16], Loss:0.4684\n",
      "Epoch[10/16], Step[1200/16], Loss:0.7427\n",
      "Epoch[10/16], Step[1400/16], Loss:0.8597\n",
      "Epoch[10/16], Step[1600/16], Loss:1.4358\n",
      "Epoch[10/16], Step[1800/16], Loss:0.8896\n",
      "Epoch[10/16], Step[2000/16], Loss:1.9479\n",
      "Epoch[10/16], Step[2200/16], Loss:1.7320\n",
      "Epoch[10/16], Step[2400/16], Loss:1.1894\n",
      "Epoch[10/16], Step[2600/16], Loss:0.3156\n",
      "Epoch[10/16], Step[2800/16], Loss:2.1184\n",
      "Epoch[10/16], Step[3000/16], Loss:1.1625\n",
      "Epoch[10/16], Step[3200/16], Loss:0.8856\n",
      "Epoch[10/16], Step[3400/16], Loss:1.9398\n",
      "Epoch[10/16], Step[3600/16], Loss:0.7396\n",
      "Epoch[10/16], Step[3800/16], Loss:1.1620\n",
      "Epoch[10/16], Step[4000/16], Loss:2.1082\n",
      "Epoch[10/16], Step[4200/16], Loss:1.8048\n",
      "Epoch[10/16], Step[4400/16], Loss:0.6412\n",
      "Epoch[10/16], Step[4600/16], Loss:1.1387\n",
      "Epoch[10/16], Step[4800/16], Loss:0.8367\n",
      "Epoch[10/16], Step[5000/16], Loss:0.7307\n",
      "Epoch[10/16], Step[5200/16], Loss:1.2758\n",
      "Epoch[10/16], Step[5400/16], Loss:2.9882\n",
      "Epoch[10/16], Step[5600/16], Loss:1.1963\n",
      "Epoch[10/16], Step[5800/16], Loss:1.2086\n",
      "Epoch[10/16], Step[6000/16], Loss:1.0920\n",
      "Epoch[10/16], Step[6200/16], Loss:0.9085\n",
      "Epoch[10/16], Step[6400/16], Loss:0.7389\n",
      "Epoch[10/16], Step[6600/16], Loss:0.5553\n",
      "Epoch[10/16], Step[6800/16], Loss:1.6904\n",
      "Epoch[10/16], Step[7000/16], Loss:1.0732\n",
      "Epoch[10/16], Step[7200/16], Loss:2.0924\n",
      "Epoch[10/16], Step[7400/16], Loss:1.5307\n",
      "Epoch[10/16], Step[7600/16], Loss:0.9612\n",
      "Epoch[10/16], Step[7800/16], Loss:1.7278\n",
      "Epoch[10/16], Step[8000/16], Loss:0.8102\n",
      "Epoch[10/16], Step[8200/16], Loss:1.2760\n",
      "Epoch[10/16], Step[8400/16], Loss:1.9265\n",
      "Epoch[10/16], Step[8600/16], Loss:1.2129\n",
      "Epoch[10/16], Step[8800/16], Loss:0.3983\n",
      "Epoch[10/16], Step[9000/16], Loss:0.5523\n",
      "Epoch[10/16], Step[9200/16], Loss:2.2700\n",
      "Epoch[10/16], Step[9400/16], Loss:0.4797\n",
      "Epoch[10/16], Step[9600/16], Loss:1.0717\n",
      "Epoch[10/16], Step[9800/16], Loss:1.4206\n",
      "Epoch[10/16], Step[10000/16], Loss:1.2787\n",
      "Epoch[10/16], Step[10200/16], Loss:0.9252\n",
      "Epoch[10/16], Step[10400/16], Loss:2.1052\n",
      "Epoch[10/16], Step[10600/16], Loss:1.3172\n",
      "Epoch[10/16], Step[10800/16], Loss:0.6055\n",
      "Epoch[10/16], Step[11000/16], Loss:1.5523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[10/16], Step[11200/16], Loss:1.4826\n",
      "Epoch[10/16], Step[11400/16], Loss:1.6260\n",
      "Epoch[10/16], Step[11600/16], Loss:1.7568\n",
      "Epoch[10/16], Step[11800/16], Loss:0.4667\n",
      "Epoch[10/16], Step[12000/16], Loss:1.7590\n",
      "Epoch[10/16], Step[12200/16], Loss:1.3226\n",
      "Epoch[10/16], Step[12400/16], Loss:1.1699\n",
      "Epoch[11/16], Step[200/16], Loss:0.7705\n",
      "Epoch[11/16], Step[400/16], Loss:2.1745\n",
      "Epoch[11/16], Step[600/16], Loss:0.7651\n",
      "Epoch[11/16], Step[800/16], Loss:0.9925\n",
      "Epoch[11/16], Step[1000/16], Loss:1.6694\n",
      "Epoch[11/16], Step[1200/16], Loss:0.9832\n",
      "Epoch[11/16], Step[1400/16], Loss:0.2385\n",
      "Epoch[11/16], Step[1600/16], Loss:1.1640\n",
      "Epoch[11/16], Step[1800/16], Loss:0.3414\n",
      "Epoch[11/16], Step[2000/16], Loss:0.5713\n",
      "Epoch[11/16], Step[2200/16], Loss:0.8856\n",
      "Epoch[11/16], Step[2400/16], Loss:1.0741\n",
      "Epoch[11/16], Step[2600/16], Loss:0.4666\n",
      "Epoch[11/16], Step[2800/16], Loss:1.1746\n",
      "Epoch[11/16], Step[3000/16], Loss:0.7634\n",
      "Epoch[11/16], Step[3200/16], Loss:0.9925\n",
      "Epoch[11/16], Step[3400/16], Loss:1.0439\n",
      "Epoch[11/16], Step[3600/16], Loss:0.8599\n",
      "Epoch[11/16], Step[3800/16], Loss:1.6439\n",
      "Epoch[11/16], Step[4000/16], Loss:2.2269\n",
      "Epoch[11/16], Step[4200/16], Loss:1.5793\n",
      "Epoch[11/16], Step[4400/16], Loss:1.4741\n",
      "Epoch[11/16], Step[4600/16], Loss:0.6919\n",
      "Epoch[11/16], Step[4800/16], Loss:2.2372\n",
      "Epoch[11/16], Step[5000/16], Loss:1.5468\n",
      "Epoch[11/16], Step[5200/16], Loss:1.8054\n",
      "Epoch[11/16], Step[5400/16], Loss:0.5781\n",
      "Epoch[11/16], Step[5600/16], Loss:0.7005\n",
      "Epoch[11/16], Step[5800/16], Loss:1.0296\n",
      "Epoch[11/16], Step[6000/16], Loss:0.5039\n",
      "Epoch[11/16], Step[6200/16], Loss:2.3676\n",
      "Epoch[11/16], Step[6400/16], Loss:0.9432\n",
      "Epoch[11/16], Step[6600/16], Loss:1.6191\n",
      "Epoch[11/16], Step[6800/16], Loss:0.8357\n",
      "Epoch[11/16], Step[7000/16], Loss:2.5675\n",
      "Epoch[11/16], Step[7200/16], Loss:0.7759\n",
      "Epoch[11/16], Step[7400/16], Loss:1.3926\n",
      "Epoch[11/16], Step[7600/16], Loss:0.3126\n",
      "Epoch[11/16], Step[7800/16], Loss:1.3046\n",
      "Epoch[11/16], Step[8000/16], Loss:0.6828\n",
      "Epoch[11/16], Step[8200/16], Loss:0.6831\n",
      "Epoch[11/16], Step[8400/16], Loss:1.4744\n",
      "Epoch[11/16], Step[8600/16], Loss:0.6446\n",
      "Epoch[11/16], Step[8800/16], Loss:1.2428\n",
      "Epoch[11/16], Step[9000/16], Loss:1.5156\n",
      "Epoch[11/16], Step[9200/16], Loss:0.5418\n",
      "Epoch[11/16], Step[9400/16], Loss:0.9042\n",
      "Epoch[11/16], Step[9600/16], Loss:2.4115\n",
      "Epoch[11/16], Step[9800/16], Loss:1.0596\n",
      "Epoch[11/16], Step[10000/16], Loss:1.3079\n",
      "Epoch[11/16], Step[10200/16], Loss:0.3142\n",
      "Epoch[11/16], Step[10400/16], Loss:1.3504\n",
      "Epoch[11/16], Step[10600/16], Loss:1.7847\n",
      "Epoch[11/16], Step[10800/16], Loss:0.9243\n",
      "Epoch[11/16], Step[11000/16], Loss:0.5812\n",
      "Epoch[11/16], Step[11200/16], Loss:1.3943\n",
      "Epoch[11/16], Step[11400/16], Loss:0.8683\n",
      "Epoch[11/16], Step[11600/16], Loss:1.0691\n",
      "Epoch[11/16], Step[11800/16], Loss:0.6207\n",
      "Epoch[11/16], Step[12000/16], Loss:1.2604\n",
      "Epoch[11/16], Step[12200/16], Loss:1.0542\n",
      "Epoch[11/16], Step[12400/16], Loss:1.0545\n",
      "Epoch[12/16], Step[200/16], Loss:1.3320\n",
      "Epoch[12/16], Step[400/16], Loss:1.8124\n",
      "Epoch[12/16], Step[600/16], Loss:0.5033\n",
      "Epoch[12/16], Step[800/16], Loss:2.5407\n",
      "Epoch[12/16], Step[1000/16], Loss:2.0262\n",
      "Epoch[12/16], Step[1200/16], Loss:1.1633\n",
      "Epoch[12/16], Step[1400/16], Loss:3.3051\n",
      "Epoch[12/16], Step[1600/16], Loss:1.2961\n",
      "Epoch[12/16], Step[1800/16], Loss:0.8875\n",
      "Epoch[12/16], Step[2000/16], Loss:0.5949\n",
      "Epoch[12/16], Step[2200/16], Loss:1.2472\n",
      "Epoch[12/16], Step[2400/16], Loss:1.3119\n",
      "Epoch[12/16], Step[2600/16], Loss:0.9904\n",
      "Epoch[12/16], Step[2800/16], Loss:1.8882\n",
      "Epoch[12/16], Step[3000/16], Loss:0.5442\n",
      "Epoch[12/16], Step[3200/16], Loss:1.4072\n",
      "Epoch[12/16], Step[3400/16], Loss:0.5731\n",
      "Epoch[12/16], Step[3600/16], Loss:2.0337\n",
      "Epoch[12/16], Step[3800/16], Loss:1.8048\n",
      "Epoch[12/16], Step[4000/16], Loss:1.8858\n",
      "Epoch[12/16], Step[4200/16], Loss:0.7724\n",
      "Epoch[12/16], Step[4400/16], Loss:2.0494\n",
      "Epoch[12/16], Step[4600/16], Loss:0.0401\n",
      "Epoch[12/16], Step[4800/16], Loss:0.8938\n",
      "Epoch[12/16], Step[5000/16], Loss:1.7669\n",
      "Epoch[12/16], Step[5200/16], Loss:1.7051\n",
      "Epoch[12/16], Step[5400/16], Loss:0.4412\n",
      "Epoch[12/16], Step[5600/16], Loss:0.9093\n",
      "Epoch[12/16], Step[5800/16], Loss:1.4593\n",
      "Epoch[12/16], Step[6000/16], Loss:1.1601\n",
      "Epoch[12/16], Step[6200/16], Loss:1.1887\n",
      "Epoch[12/16], Step[6400/16], Loss:1.0882\n",
      "Epoch[12/16], Step[6600/16], Loss:1.1002\n",
      "Epoch[12/16], Step[6800/16], Loss:1.4628\n",
      "Epoch[12/16], Step[7000/16], Loss:2.3868\n",
      "Epoch[12/16], Step[7200/16], Loss:1.3759\n",
      "Epoch[12/16], Step[7400/16], Loss:0.3976\n",
      "Epoch[12/16], Step[7600/16], Loss:2.0153\n",
      "Epoch[12/16], Step[7800/16], Loss:0.6581\n",
      "Epoch[12/16], Step[8000/16], Loss:0.5053\n",
      "Epoch[12/16], Step[8200/16], Loss:1.2290\n",
      "Epoch[12/16], Step[8400/16], Loss:0.4983\n",
      "Epoch[12/16], Step[8600/16], Loss:0.3201\n",
      "Epoch[12/16], Step[8800/16], Loss:1.4249\n",
      "Epoch[12/16], Step[9000/16], Loss:3.1458\n",
      "Epoch[12/16], Step[9200/16], Loss:1.8593\n",
      "Epoch[12/16], Step[9400/16], Loss:1.5727\n",
      "Epoch[12/16], Step[9600/16], Loss:0.7496\n",
      "Epoch[12/16], Step[9800/16], Loss:1.3884\n",
      "Epoch[12/16], Step[10000/16], Loss:0.4449\n",
      "Epoch[12/16], Step[10200/16], Loss:1.3465\n",
      "Epoch[12/16], Step[10400/16], Loss:0.4954\n",
      "Epoch[12/16], Step[10600/16], Loss:0.6663\n",
      "Epoch[12/16], Step[10800/16], Loss:1.4387\n",
      "Epoch[12/16], Step[11000/16], Loss:1.1172\n",
      "Epoch[12/16], Step[11200/16], Loss:0.5664\n",
      "Epoch[12/16], Step[11400/16], Loss:0.9992\n",
      "Epoch[12/16], Step[11600/16], Loss:1.1438\n",
      "Epoch[12/16], Step[11800/16], Loss:0.7015\n",
      "Epoch[12/16], Step[12000/16], Loss:0.7190\n",
      "Epoch[12/16], Step[12200/16], Loss:0.7247\n",
      "Epoch[12/16], Step[12400/16], Loss:1.1875\n",
      "Epoch[13/16], Step[200/16], Loss:0.7639\n",
      "Epoch[13/16], Step[400/16], Loss:0.5666\n",
      "Epoch[13/16], Step[600/16], Loss:0.4669\n",
      "Epoch[13/16], Step[800/16], Loss:2.1051\n",
      "Epoch[13/16], Step[1000/16], Loss:0.6890\n",
      "Epoch[13/16], Step[1200/16], Loss:1.0991\n",
      "Epoch[13/16], Step[1400/16], Loss:1.1330\n",
      "Epoch[13/16], Step[1600/16], Loss:0.7010\n",
      "Epoch[13/16], Step[1800/16], Loss:0.4060\n",
      "Epoch[13/16], Step[2000/16], Loss:2.7753\n",
      "Epoch[13/16], Step[2200/16], Loss:1.8522\n",
      "Epoch[13/16], Step[2400/16], Loss:1.1323\n",
      "Epoch[13/16], Step[2600/16], Loss:1.0155\n",
      "Epoch[13/16], Step[2800/16], Loss:1.5202\n",
      "Epoch[13/16], Step[3000/16], Loss:0.3545\n",
      "Epoch[13/16], Step[3200/16], Loss:0.4457\n",
      "Epoch[13/16], Step[3400/16], Loss:0.6147\n",
      "Epoch[13/16], Step[3600/16], Loss:0.2767\n",
      "Epoch[13/16], Step[3800/16], Loss:1.8027\n",
      "Epoch[13/16], Step[4000/16], Loss:0.5787\n",
      "Epoch[13/16], Step[4200/16], Loss:0.8613\n",
      "Epoch[13/16], Step[4400/16], Loss:1.0978\n",
      "Epoch[13/16], Step[4600/16], Loss:1.2270\n",
      "Epoch[13/16], Step[4800/16], Loss:1.4197\n",
      "Epoch[13/16], Step[5000/16], Loss:1.6937\n",
      "Epoch[13/16], Step[5200/16], Loss:1.5069\n",
      "Epoch[13/16], Step[5400/16], Loss:2.5621\n",
      "Epoch[13/16], Step[5600/16], Loss:1.6392\n",
      "Epoch[13/16], Step[5800/16], Loss:0.7780\n",
      "Epoch[13/16], Step[6000/16], Loss:0.5939\n",
      "Epoch[13/16], Step[6200/16], Loss:0.8858\n",
      "Epoch[13/16], Step[6400/16], Loss:1.2225\n",
      "Epoch[13/16], Step[6600/16], Loss:1.0420\n",
      "Epoch[13/16], Step[6800/16], Loss:1.5651\n",
      "Epoch[13/16], Step[7000/16], Loss:0.9064\n",
      "Epoch[13/16], Step[7200/16], Loss:1.5031\n",
      "Epoch[13/16], Step[7400/16], Loss:1.3781\n",
      "Epoch[13/16], Step[7600/16], Loss:1.3356\n",
      "Epoch[13/16], Step[7800/16], Loss:1.1796\n",
      "Epoch[13/16], Step[8000/16], Loss:1.3985\n",
      "Epoch[13/16], Step[8200/16], Loss:0.2964\n",
      "Epoch[13/16], Step[8400/16], Loss:0.9968\n",
      "Epoch[13/16], Step[8600/16], Loss:0.1526\n",
      "Epoch[13/16], Step[8800/16], Loss:1.0575\n",
      "Epoch[13/16], Step[9000/16], Loss:0.8948\n",
      "Epoch[13/16], Step[9200/16], Loss:1.1704\n",
      "Epoch[13/16], Step[9400/16], Loss:1.5214\n",
      "Epoch[13/16], Step[9600/16], Loss:1.5610\n",
      "Epoch[13/16], Step[9800/16], Loss:1.1762\n",
      "Epoch[13/16], Step[10000/16], Loss:1.1488\n",
      "Epoch[13/16], Step[10200/16], Loss:0.7730\n",
      "Epoch[13/16], Step[10400/16], Loss:1.6405\n",
      "Epoch[13/16], Step[10600/16], Loss:1.4254\n",
      "Epoch[13/16], Step[10800/16], Loss:1.1091\n",
      "Epoch[13/16], Step[11000/16], Loss:0.6876\n",
      "Epoch[13/16], Step[11200/16], Loss:2.0198\n",
      "Epoch[13/16], Step[11400/16], Loss:0.2921\n",
      "Epoch[13/16], Step[11600/16], Loss:1.2365\n",
      "Epoch[13/16], Step[11800/16], Loss:1.8568\n",
      "Epoch[13/16], Step[12000/16], Loss:1.1798\n",
      "Epoch[13/16], Step[12200/16], Loss:1.5670\n",
      "Epoch[13/16], Step[12400/16], Loss:0.5180\n",
      "Epoch[14/16], Step[200/16], Loss:1.7241\n",
      "Epoch[14/16], Step[400/16], Loss:0.4609\n",
      "Epoch[14/16], Step[600/16], Loss:1.0394\n",
      "Epoch[14/16], Step[800/16], Loss:1.1987\n",
      "Epoch[14/16], Step[1000/16], Loss:0.6994\n",
      "Epoch[14/16], Step[1200/16], Loss:1.1764\n",
      "Epoch[14/16], Step[1400/16], Loss:1.2468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[14/16], Step[1600/16], Loss:1.5738\n",
      "Epoch[14/16], Step[1800/16], Loss:0.7958\n",
      "Epoch[14/16], Step[2000/16], Loss:0.7888\n",
      "Epoch[14/16], Step[2200/16], Loss:1.9394\n",
      "Epoch[14/16], Step[2400/16], Loss:1.1013\n",
      "Epoch[14/16], Step[2600/16], Loss:1.2446\n",
      "Epoch[14/16], Step[2800/16], Loss:0.9514\n",
      "Epoch[14/16], Step[3000/16], Loss:1.2535\n",
      "Epoch[14/16], Step[3200/16], Loss:2.1969\n",
      "Epoch[14/16], Step[3400/16], Loss:1.0213\n",
      "Epoch[14/16], Step[3600/16], Loss:1.4151\n",
      "Epoch[14/16], Step[3800/16], Loss:2.5871\n",
      "Epoch[14/16], Step[4000/16], Loss:1.3798\n",
      "Epoch[14/16], Step[4200/16], Loss:1.5631\n",
      "Epoch[14/16], Step[4400/16], Loss:0.4678\n",
      "Epoch[14/16], Step[4600/16], Loss:1.2012\n",
      "Epoch[14/16], Step[4800/16], Loss:0.4157\n",
      "Epoch[14/16], Step[5000/16], Loss:0.7935\n",
      "Epoch[14/16], Step[5200/16], Loss:0.8380\n",
      "Epoch[14/16], Step[5400/16], Loss:1.1922\n",
      "Epoch[14/16], Step[5600/16], Loss:0.6239\n",
      "Epoch[14/16], Step[5800/16], Loss:1.2432\n",
      "Epoch[14/16], Step[6000/16], Loss:0.9780\n",
      "Epoch[14/16], Step[6200/16], Loss:1.4038\n",
      "Epoch[14/16], Step[6400/16], Loss:1.7327\n",
      "Epoch[14/16], Step[6600/16], Loss:1.7697\n",
      "Epoch[14/16], Step[6800/16], Loss:0.4826\n",
      "Epoch[14/16], Step[7000/16], Loss:0.7839\n",
      "Epoch[14/16], Step[7200/16], Loss:0.0814\n",
      "Epoch[14/16], Step[7400/16], Loss:0.7292\n",
      "Epoch[14/16], Step[7600/16], Loss:1.1566\n",
      "Epoch[14/16], Step[7800/16], Loss:0.9795\n",
      "Epoch[14/16], Step[8000/16], Loss:0.4015\n",
      "Epoch[14/16], Step[8200/16], Loss:1.0265\n",
      "Epoch[14/16], Step[8400/16], Loss:1.3392\n",
      "Epoch[14/16], Step[8600/16], Loss:0.9375\n",
      "Epoch[14/16], Step[8800/16], Loss:1.7069\n",
      "Epoch[14/16], Step[9000/16], Loss:0.4976\n",
      "Epoch[14/16], Step[9200/16], Loss:1.1426\n",
      "Epoch[14/16], Step[9400/16], Loss:1.6437\n",
      "Epoch[14/16], Step[9600/16], Loss:0.1964\n",
      "Epoch[14/16], Step[9800/16], Loss:0.5450\n",
      "Epoch[14/16], Step[10000/16], Loss:1.2484\n",
      "Epoch[14/16], Step[10200/16], Loss:1.2699\n",
      "Epoch[14/16], Step[10400/16], Loss:1.8918\n",
      "Epoch[14/16], Step[10600/16], Loss:0.6054\n",
      "Epoch[14/16], Step[10800/16], Loss:1.8338\n",
      "Epoch[14/16], Step[11000/16], Loss:1.9016\n",
      "Epoch[14/16], Step[11200/16], Loss:1.1253\n",
      "Epoch[14/16], Step[11400/16], Loss:1.0127\n",
      "Epoch[14/16], Step[11600/16], Loss:0.5223\n",
      "Epoch[14/16], Step[11800/16], Loss:0.8927\n",
      "Epoch[14/16], Step[12000/16], Loss:0.9930\n",
      "Epoch[14/16], Step[12200/16], Loss:1.0812\n",
      "Epoch[14/16], Step[12400/16], Loss:0.5298\n",
      "Epoch[15/16], Step[200/16], Loss:0.4691\n",
      "Epoch[15/16], Step[400/16], Loss:0.5198\n",
      "Epoch[15/16], Step[600/16], Loss:1.5099\n",
      "Epoch[15/16], Step[800/16], Loss:0.1708\n",
      "Epoch[15/16], Step[1000/16], Loss:1.4891\n",
      "Epoch[15/16], Step[1200/16], Loss:0.9426\n",
      "Epoch[15/16], Step[1400/16], Loss:0.6942\n",
      "Epoch[15/16], Step[1600/16], Loss:2.6143\n",
      "Epoch[15/16], Step[1800/16], Loss:1.2637\n",
      "Epoch[15/16], Step[2000/16], Loss:2.3200\n",
      "Epoch[15/16], Step[2200/16], Loss:0.5426\n",
      "Epoch[15/16], Step[2400/16], Loss:0.4095\n",
      "Epoch[15/16], Step[2600/16], Loss:0.8556\n",
      "Epoch[15/16], Step[2800/16], Loss:0.4822\n",
      "Epoch[15/16], Step[3000/16], Loss:2.6170\n",
      "Epoch[15/16], Step[3200/16], Loss:0.5612\n",
      "Epoch[15/16], Step[3400/16], Loss:0.4619\n",
      "Epoch[15/16], Step[3600/16], Loss:1.5227\n",
      "Epoch[15/16], Step[3800/16], Loss:1.0947\n",
      "Epoch[15/16], Step[4000/16], Loss:1.2488\n",
      "Epoch[15/16], Step[4200/16], Loss:0.3578\n",
      "Epoch[15/16], Step[4400/16], Loss:0.5021\n",
      "Epoch[15/16], Step[4600/16], Loss:1.6552\n",
      "Epoch[15/16], Step[4800/16], Loss:1.0845\n",
      "Epoch[15/16], Step[5000/16], Loss:1.6981\n",
      "Epoch[15/16], Step[5200/16], Loss:0.1945\n",
      "Epoch[15/16], Step[5400/16], Loss:0.2460\n",
      "Epoch[15/16], Step[5600/16], Loss:1.4227\n",
      "Epoch[15/16], Step[5800/16], Loss:1.0281\n",
      "Epoch[15/16], Step[6000/16], Loss:0.5612\n",
      "Epoch[15/16], Step[6200/16], Loss:1.3216\n",
      "Epoch[15/16], Step[6400/16], Loss:1.2432\n",
      "Epoch[15/16], Step[6600/16], Loss:0.8087\n",
      "Epoch[15/16], Step[6800/16], Loss:0.1162\n",
      "Epoch[15/16], Step[7000/16], Loss:0.9902\n",
      "Epoch[15/16], Step[7200/16], Loss:0.8237\n",
      "Epoch[15/16], Step[7400/16], Loss:0.3003\n",
      "Epoch[15/16], Step[7600/16], Loss:0.6138\n",
      "Epoch[15/16], Step[7800/16], Loss:0.3572\n",
      "Epoch[15/16], Step[8000/16], Loss:1.2795\n",
      "Epoch[15/16], Step[8200/16], Loss:0.6897\n",
      "Epoch[15/16], Step[8400/16], Loss:1.5929\n",
      "Epoch[15/16], Step[8600/16], Loss:0.8455\n",
      "Epoch[15/16], Step[8800/16], Loss:0.9028\n",
      "Epoch[15/16], Step[9000/16], Loss:1.1471\n",
      "Epoch[15/16], Step[9200/16], Loss:0.8924\n",
      "Epoch[15/16], Step[9400/16], Loss:0.0621\n",
      "Epoch[15/16], Step[9600/16], Loss:1.6530\n",
      "Epoch[15/16], Step[9800/16], Loss:1.3997\n",
      "Epoch[15/16], Step[10000/16], Loss:0.4796\n",
      "Epoch[15/16], Step[10200/16], Loss:0.8415\n",
      "Epoch[15/16], Step[10400/16], Loss:0.0927\n",
      "Epoch[15/16], Step[10600/16], Loss:0.7911\n",
      "Epoch[15/16], Step[10800/16], Loss:1.3310\n",
      "Epoch[15/16], Step[11000/16], Loss:1.7709\n",
      "Epoch[15/16], Step[11200/16], Loss:1.2540\n",
      "Epoch[15/16], Step[11400/16], Loss:1.1918\n",
      "Epoch[15/16], Step[11600/16], Loss:0.7073\n",
      "Epoch[15/16], Step[11800/16], Loss:0.6211\n",
      "Epoch[15/16], Step[12000/16], Loss:2.6297\n",
      "Epoch[15/16], Step[12200/16], Loss:0.3486\n",
      "Epoch[15/16], Step[12400/16], Loss:0.8277\n",
      "Epoch[16/16], Step[200/16], Loss:0.2272\n",
      "Epoch[16/16], Step[400/16], Loss:1.2668\n",
      "Epoch[16/16], Step[600/16], Loss:0.8453\n",
      "Epoch[16/16], Step[800/16], Loss:0.7735\n",
      "Epoch[16/16], Step[1000/16], Loss:0.6525\n",
      "Epoch[16/16], Step[1200/16], Loss:0.4859\n",
      "Epoch[16/16], Step[1400/16], Loss:0.5106\n",
      "Epoch[16/16], Step[1600/16], Loss:0.7224\n",
      "Epoch[16/16], Step[1800/16], Loss:0.9549\n",
      "Epoch[16/16], Step[2000/16], Loss:1.3056\n",
      "Epoch[16/16], Step[2200/16], Loss:1.1544\n",
      "Epoch[16/16], Step[2400/16], Loss:0.9143\n",
      "Epoch[16/16], Step[2600/16], Loss:0.7882\n",
      "Epoch[16/16], Step[2800/16], Loss:0.4905\n",
      "Epoch[16/16], Step[3000/16], Loss:0.8569\n",
      "Epoch[16/16], Step[3200/16], Loss:0.7677\n",
      "Epoch[16/16], Step[3400/16], Loss:0.4853\n",
      "Epoch[16/16], Step[3600/16], Loss:1.2763\n",
      "Epoch[16/16], Step[3800/16], Loss:0.7983\n",
      "Epoch[16/16], Step[4000/16], Loss:0.5121\n",
      "Epoch[16/16], Step[4200/16], Loss:0.4973\n",
      "Epoch[16/16], Step[4400/16], Loss:1.8699\n",
      "Epoch[16/16], Step[4600/16], Loss:1.4236\n",
      "Epoch[16/16], Step[4800/16], Loss:0.8507\n",
      "Epoch[16/16], Step[5000/16], Loss:1.4637\n",
      "Epoch[16/16], Step[5200/16], Loss:1.0568\n",
      "Epoch[16/16], Step[5400/16], Loss:1.2037\n",
      "Epoch[16/16], Step[5600/16], Loss:1.2790\n",
      "Epoch[16/16], Step[5800/16], Loss:1.2975\n",
      "Epoch[16/16], Step[6000/16], Loss:1.4793\n",
      "Epoch[16/16], Step[6200/16], Loss:0.2727\n",
      "Epoch[16/16], Step[6400/16], Loss:0.5984\n",
      "Epoch[16/16], Step[6600/16], Loss:0.0879\n",
      "Epoch[16/16], Step[6800/16], Loss:1.2387\n",
      "Epoch[16/16], Step[7000/16], Loss:0.2636\n",
      "Epoch[16/16], Step[7200/16], Loss:1.5063\n",
      "Epoch[16/16], Step[7400/16], Loss:0.5256\n",
      "Epoch[16/16], Step[7600/16], Loss:0.9354\n",
      "Epoch[16/16], Step[7800/16], Loss:0.1155\n",
      "Epoch[16/16], Step[8000/16], Loss:0.7363\n",
      "Epoch[16/16], Step[8200/16], Loss:0.5202\n",
      "Epoch[16/16], Step[8400/16], Loss:0.2076\n",
      "Epoch[16/16], Step[8600/16], Loss:0.5985\n",
      "Epoch[16/16], Step[8800/16], Loss:0.9893\n",
      "Epoch[16/16], Step[9000/16], Loss:0.9896\n",
      "Epoch[16/16], Step[9200/16], Loss:0.6119\n",
      "Epoch[16/16], Step[9400/16], Loss:1.7099\n",
      "Epoch[16/16], Step[9600/16], Loss:2.6214\n",
      "Epoch[16/16], Step[9800/16], Loss:1.3418\n",
      "Epoch[16/16], Step[10000/16], Loss:1.0093\n",
      "Epoch[16/16], Step[10200/16], Loss:0.4568\n",
      "Epoch[16/16], Step[10400/16], Loss:1.0141\n",
      "Epoch[16/16], Step[10600/16], Loss:1.5644\n",
      "Epoch[16/16], Step[10800/16], Loss:0.7172\n",
      "Epoch[16/16], Step[11000/16], Loss:1.0356\n",
      "Epoch[16/16], Step[11200/16], Loss:1.8170\n",
      "Epoch[16/16], Step[11400/16], Loss:1.0454\n",
      "Epoch[16/16], Step[11600/16], Loss:0.3351\n",
      "Epoch[16/16], Step[11800/16], Loss:1.4155\n",
      "Epoch[16/16], Step[12000/16], Loss:1.1371\n",
      "Epoch[16/16], Step[12200/16], Loss:1.1989\n",
      "Epoch[16/16], Step[12400/16], Loss:0.5678\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i , (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Convert one-hot encoded labels to class indices\n",
    "        #labels = torch.argmax(labels, dim=1).long()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 200 == 0:\n",
    "            print(f'Epoch[{epoch+1}/{num_epochs}], Step[{i+1}/{num_epochs}], Loss:{loss.item():.4f}')\n",
    "        \n",
    "print(\"Finished Training\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "35a73860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i, (images, labels) in enumerate(train_loader):\n",
    "#    print(images.size(), labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "11c06ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the network = 62.52 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0  for i in range(10)]\n",
    "    n_class_samples = [0  for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # max returns (value, index)\n",
    "        _, predicted =  torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted==labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred =  predicted[i]\n",
    "            \n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "            \n",
    "acc = 100.0 * n_correct/n_samples\n",
    "\n",
    "print(f'accuracy of the network = {acc} %')\n",
    "                \n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4443bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of Airplane = 7.13 %\n",
      "accuracy of Automobile = 7.21 %\n",
      "accuracy of Bird = 4.25 %\n",
      "accuracy of Cat = 3.85 %\n",
      "accuracy of Deer = 5.85 %\n",
      "accuracy of Dog = 5.29 %\n",
      "accuracy of Frog = 6.95 %\n",
      "accuracy of Horse = 7.5 %\n",
      "accuracy of Ship = 7.23 %\n",
      "accuracy of Truck = 7.26 %\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    acc = 100.0 * n_class_correct[i]/n_samples\n",
    "    print(f'accuracy of {classes[i]} = {acc} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef12d308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620f7708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
